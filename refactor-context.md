You are an expert macOS + Python AI engineer.

I want you to completely remove everything related to the old TTS models â€” Coqui XTTS, Bark, and OpenVoice â€” from my Flask-based local TTS project.  
That includes all:
- Old dependencies (TTS, Bark, OpenVoice)
- Any references in setup scripts, requirements, or code
- Unused virtual environments and model loaders

Then rebuild the entire project **from scratch** using a modern modular architecture that supports **multi-speaker voice selection**.

---

## ğŸ§± 1ï¸âƒ£ Environment Setup

- Create a fresh `venv` using **Python 3.10** (compatible with all target models).
- Detect mac architecture (Apple Silicon `arm64` vs Intel `x86_64`).
- Install cleanly:
  - `torch` (Metal backend for Apple Silicon or CPU for Intel)
  - `flask`, `soundfile`, `accelerate`, `numpy`, `requests`
  - Any model-specific dependencies for Kokoro, Zonos, Chatterbox, DIA, Orpheus, and CSM
- Automatically download or install the models from their sources (HuggingFace / GitHub)
- Save a `requirements-neo.txt` snapshot
- File: `setup-neo-tts.sh`

---

## âš™ï¸ 2ï¸âƒ£ Flask App (app/app.py)

Rebuild the Flask app to:
- Load models dynamically (only one active at a time)
- Detect available **voices/speakers** per model (multi-speaker models)
- Provide the following functionality:
  - Dropdown to select model
  - Dropdown to select voice (auto-populated once model is selected)
  - Text input for the user
  - â€œGenerateâ€ button to synthesize speech
  - Auto-play or download the resulting audio file

All synthesis should happen via **POST** requests.

Save generated audio to `static/output.wav` and display a player after generation.

Each synthesis should be logged in `logs/results.csv` with:
timestamp, model, speaker, text, duration, output_path

yaml
Copy code

---

## ğŸ§© 3ï¸âƒ£ Modular Model Loader Architecture

Create a `models/` folder with one file per model:
models/
â”œâ”€â”€ kokoro.py
â”œâ”€â”€ chatterbox.py
â”œâ”€â”€ zonos.py
â”œâ”€â”€ dia.py
â”œâ”€â”€ orpheus.py
â”œâ”€â”€ csm.py

python
Copy code

Each file must expose a consistent interface:
```python
def list_voices() -> list:
    """Return a list of available speakers/voices for this model."""

def generate_audio(text: str, voice: str = None, output_path: str = "static/output.wav") -> str:
    """Generate audio for given text and optional voice, returning the saved file path."""
ğŸ—£ï¸ 4ï¸âƒ£ Multi-Speaker Voice Selection Logic
Kokoro: use available voice presets (male, female, etc.)

Chatterbox: list internal trained voices (e.g., p225, p226, etc.)

Zonos: allow the user to upload or specify a reference voice file for cloning (speaker_wav)

DIA, Orpheus, CSM: if single-speaker, disable the voice dropdown dynamically

When the user selects a model, fetch available voices from /get_voices?model=xxx endpoint and populate the dropdown.

ğŸ’» 5ï¸âƒ£ Frontend (templates/index.html)
Build a clean, minimal web UI (Tailwind or basic CSS):

Model dropdown

Voice dropdown (hidden/disabled for single-speaker models)

Text input area

Generate button

Embedded audio player (auto-updates after generation)

Optional small log display (show last generated model/voice)

Example layout:

less
Copy code
Select Model: [ Kokoro â–¼ ]
Select Voice: [ voice_1 â–¼ ]
Enter Text: [ "Hello world..." ]
[Generate ğŸ”Š]
[ Player â¯ï¸ ]
Use JavaScript to:

Dynamically fetch voices when model changes

Send POST requests for synthesis

Update the player without reloading the page

ğŸ§  6ï¸âƒ£ Model Management Details
Kokoro: Fast lightweight TTS for local mac (supports multiple voices)

Chatterbox: Conversational expressive model with multiple speakers

Zonos: Zero-shot voice cloning (optional user-uploaded voice file)

DIA: High-quality expressive model for narration

Orpheus: Singing / creative synthesis (optional mode)

CSM: Optional contextual module enhancing voice consistency

All models should be fully offline â€” no API calls.

ğŸ§¾ 7ï¸âƒ£ Verification
At the end of the setup script:

Test-generate short audio for each model (e.g. â€œHello from [model name]â€)

Print confirmation for each model:

Copy code
âœ… Kokoro OK
âœ… Chatterbox OK
âœ… Zonos OK
âœ… DIA OK
âœ… Orpheus OK
âœ… CSM OK
ğŸ§° 8ï¸âƒ£ Deliverables
Final project structure:

project-root/
â”‚
â”œâ”€â”€ setup-neo-tts.sh                # One-click setup script (Python 3.10 + models)
â”œâ”€â”€ venv/                           # Virtual environment (auto-created)
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ app.py                      # Main Flask server
â”‚   â”‚
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ index.html              # Frontend UI (model + voice selector)
â”‚   â”‚
â”‚   â”œâ”€â”€ static/                     # Flask static directory
â”‚   â”‚   â”œâ”€â”€ output/                 # Generated audio files
â”‚   â”‚   â”‚   â””â”€â”€ output.wav
â”‚   â”‚   â”œâ”€â”€ css/                    # (Optional) Tailwind or custom styles
â”‚   â”‚   â””â”€â”€ js/                     # (Optional) Frontend JS for dynamic updates
â”‚   â”‚
â”‚   â””â”€â”€ __init__.py                 # (Optional) Makes `app/` importable
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ kokoro.py                   # Base lightweight TTS model
â”‚   â”œâ”€â”€ chatterbox.py               # Conversational expressive model
â”‚   â”œâ”€â”€ zonos.py                    # Zero-shot voice cloning
â”‚   â”œâ”€â”€ dia.py                      # Emotional intonation model
â”‚   â”œâ”€â”€ orpheus.py                  # Singing/creative synthesis
â”‚   â””â”€â”€ csm.py                      # Contextual speaker model (enhancer)
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ results.csv                 # Logs: timestamp, model, voice, text, duration
â”‚
â””â”€â”€ requirements-neo.txt            # Frozen environment for reproducibility

ğŸ¯ End Goal
After setup, I should be able to:

Run bash setup-neo-tts.sh

Activate the virtualenv: source venv/bin/activate

Start the Flask server: python app/app.py

Visit http://localhost:5000

Select a model, choose a voice, type text, and play the generated output.

All generation must be local, modular, and logged.

ğŸ”¥ Notes
Use modern Python best practices.

Keep each model loader modular and clean.

Ensure voice lists are dynamically fetched and cached.

Avoid large runtime dependencies that break macOS portability.